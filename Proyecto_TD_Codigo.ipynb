{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dgahufnp8VC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Inicializar spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Cargar el modelo BERT y el tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Asegurarse de que el modelo esté en modo de evaluación\n",
        "model.eval()\n",
        "\n",
        "# Ruta al archivo JSON (ajusta el path según donde esté ubicado el archivo)\n",
        "path_file = \"full_format_recipes.json\"\n",
        "\n",
        "# Cargar el archivo JSON como un DataFrame de Pandas\n",
        "def load_recipes(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as file:\n",
        "            recipes_df = pd.read_json(file)\n",
        "            recipes_df.dropna(inplace=True)\n",
        "            print(\"Datos cargados exitosamente.\")\n",
        "            return recipes_df\n",
        "    except ValueError as e:\n",
        "        print(f\"Error al leer el archivo JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text_spacy(text):\n",
        "\n",
        "    # Eliminar caracteres no alfabéticos\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Procesar el texto con SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenización y Lematización con SpaCy\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    text_lemma = ' '.join(tokens)\n",
        "\n",
        "    # Convertimos el texto en una lista de tokens\n",
        "    words = re.findall(r'\\S+', text_lemma)\n",
        "\n",
        "    # Devolver el texto procesado como lista de palabras\n",
        "    return words\n",
        "\n",
        "def compute_tfidf(docs):\n",
        "\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
        "    return tfidf_matrix, vectorizer\n",
        "\n",
        "def compute_word2vec(docs):\n",
        "\n",
        "    # Tokenización de documentos\n",
        "    tokenized_docs = [doc.split() for doc in docs]\n",
        "\n",
        "    # Entrenamiento de Word2Vec\n",
        "    model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Calcular promedio de embeddings para cada documento\n",
        "    doc_vectors = []\n",
        "    for tokens in tokenized_docs:\n",
        "        word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "        if word_vectors:\n",
        "            doc_vectors.append(np.mean(word_vectors, axis=0))\n",
        "        else:\n",
        "            doc_vectors.append(np.zeros(model.vector_size))\n",
        "\n",
        "    return np.array(doc_vectors), model\n",
        "\n",
        "\n",
        "def compute_bert_embeddings(docs, batch_size=16): # Usa un bucle que procese\n",
        "    embeddings = []                                     # los documentos en lotes pequeños (por ejemplo, de 16 o 32 documentos a la vez).\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        batch = docs[i:i + batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "        embeddings.append(batch_embeddings)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "\n",
        "# Cargar recetas\n",
        "recipes_df = load_recipes(path_file)\n",
        "\n",
        "# Tomar una muestra de las primeras 20 filas para prueba\n",
        "recipes_df = recipes_df.iloc[0:100, :]\n",
        "\n",
        "# Obtener la columna 'desc' (descripción de las recetas)\n",
        "text_to_process = recipes_df.loc[:, \"desc\"]\n",
        "ratings = recipes_df.loc[:, \"rating\"]\n",
        "\n",
        "# Aplicar el preprocesamiento de texto a cada descripción\n",
        "processed_words = text_to_process.apply(preprocess_text_spacy)\n",
        "\n",
        "# Convertir la lista de palabras procesadas a un formato adecuado para TF-IDF y Word2Vec (como lista de strings)\n",
        "processed_docs = [' '.join(words) for words in processed_words]\n",
        "\n",
        "# 1. Aplicar TF-IDF\n",
        "tfidf_matrix, vectorizer = compute_tfidf(processed_docs)\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "matrix=tfidf_matrix.toarray()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. Aplicar Word2Vec\n",
        "word2vec_vectors, word2vec_model = compute_word2vec(processed_docs)\n",
        "#print(\"Vectores promedio de Word2Vec:\")\n",
        "#print(word2vec_vectors)\n",
        "\n",
        "plt.figure(2)\n",
        "# Plotear los vectores\n",
        "plt.scatter(word2vec_vectors[:, 0], word2vec_vectors[:, 1])\n",
        "plt.title(\"Representación de recetas con Word2Vec (PCA)\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Aplicar BERT\n",
        "# Convertir las descripciones a una lista de documentos (sin preprocesar)\n",
        "processed_docs = text_to_process.tolist()\n",
        "\n",
        "# Obtener los embeddings de BERT para las descripciones\n",
        "bert_embeddings = compute_bert_embeddings(processed_docs)\n",
        "#Verificar dimensión de la matriz de embeddings.\n",
        "print(bert_embeddings.shape)  # (batch_size, embedding_dim)\n",
        "# Guardar embeddings en un archivo\n",
        "np.save(\"bert_embeddings.npy\", bert_embeddings)\n",
        "\n",
        "# Cargar embeddings desde el archivo\n",
        "#bert_embeddings = np.load(\"bert_embeddings.npy\")\n",
        "\n",
        "\"\"\"\n",
        "# Mostrar los embeddings de BERT para los primeros 3 documentos\n",
        "print(\"Embeddings de BERT:\")\n",
        "print(bert_embeddings[:3])  # Muestra los primeros 3 embeddings\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#######################################\n",
        "#Redes Neuronales utilizando PyTorch\n",
        "#######################################\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 4. Entrenamiento y evaluaci´on de modelos de regresión\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Estandarizar los datos (si es necesario para KNN, ya que KNN depende de las distancias entre puntos)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#####\n",
        "###  Regresor K-NN  ###\n",
        "#####\n",
        "\n",
        "def KNN_Regresor(X_train_scaled,X_test_scaled,y_train,n_neighbors=10):\n",
        "\n",
        "    # Entrenamiento del modelo KNN Regressor\n",
        "    knn_model = KNeighborsRegressor(n_neighbors)  # Puedes ajustar 'n_neighbors' según sea necesario\n",
        "    knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predicción y evaluación del modelo\n",
        "    y_pred_knn = knn_model.predict(X_test_scaled)\n",
        "\n",
        "    # Calcular el error cuadrático medio (MSE) para KNN\n",
        "    mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "    print(f\"KNN Regressor MSE: {mse_knn}\")\n",
        "\n",
        "    # Calcular mae para KNN\n",
        "    mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
        "\n",
        "    print(f\"KNN Regressor MAE: {mae_knn}\")\n",
        "\n",
        "    return y_pred_knn\n",
        "\n",
        "y_pred_knn=KNN_Regresor(X_train_scaled,X_test_scaled,y_train,n_neighbors=5)\n",
        "\n",
        "######\n",
        "###  Regresión_NN con pytorch ###\n",
        "######\n",
        "# Definir el modelo de red neuronal\n",
        "class RegressionNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)  # Capa de entrada\n",
        "        self.fc2 = nn.Linear(128, 64)  # Capa oculta\n",
        "        self.fc3 = nn.Linear(64, 1)  # Capa de salida (valor de rating)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Convertir los datos a tensores de PyTorch\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Inicializar el modelo\n",
        "model_nn = RegressionNN(input_size=X_train_scaled.shape[1])\n",
        "\n",
        "# Definir la función de pérdida y el optimizador\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_nn.parameters(), lr=1e-3)\n",
        "\n",
        "# Entrenamiento de la red neuronal\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model_nn(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass y optimización\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #if (epoch+1) % 10 == 0:\n",
        "        # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "# Predicción y evaluación del modelo\n",
        "model_nn.eval()  # Poner el modelo en modo evaluación\n",
        "with torch.no_grad():\n",
        "    y_pred_nn = model_nn(X_test_tensor)\n",
        "\n",
        "# Calcular el error cuadrático medio (MSE) para la red neuronal\n",
        "mse_nn = mean_squared_error(y_test_tensor.numpy(), y_pred_nn.numpy())\n",
        "print(f\"Neural Network MSE: {mse_nn}\")\n",
        "# Calcular mae para NN\n",
        "mae_nn = mean_absolute_error(y_test_tensor.numpy(), y_pred_nn.numpy())\n",
        "print(f\"Neural Network MAE: {mae_nn}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred_knn, color='blue', label=\"KNN Predictions\", alpha=0.6)\n",
        "plt.scatter(y_test, y_pred_nn, color='red', label=\"NN Predictions\", alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='green', linestyle='--', label=\"Ideal Fit\")\n",
        "plt.xlabel(\"True Ratings\")\n",
        "plt.ylabel(\"Predicted Ratings\")\n",
        "plt.title(\"KNN vs Neural Network Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#####\n",
        "#5. Comparación KNN, NN y Fine-Tuning BER\n",
        "#####\n",
        "\n",
        "\n",
        "# Suponiendo que ya tienes los embeddings de BERT precalificados en 'bert_embeddings.npy'\n",
        "# Cargar los embeddings de BERT\n",
        "bert_embeddings = np.load(\"bert_embeddings.npy\")\n",
        "\n",
        "# Cargar las calificaciones (ratings)\n",
        "ratings = recipes_df.loc[:, \"rating\"].values\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Estandarizar los datos (es necesario para KNN y regresores)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convertir los datos a tensores de PyTorch\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Definir el modelo de regresión (una simple red neuronal con una capa densa)\n",
        "class FineTunedBERT(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FineTunedBERT, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_dim, 256)  # Capa de entrada\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dense2 = nn.Linear(256, 1)  # Capa de salida (valor de rating)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "# Inicializar el modelo\n",
        "input_dim = X_train_scaled.shape[1]  # Número de características de los embeddings de BERT\n",
        "model = FineTunedBERT(input_dim)\n",
        "\n",
        "# Definir la función de pérdida y el optimizador\n",
        "criterion = nn.MSELoss()  # Usamos MSELoss para una tarea de regresión\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train_tensor)\n",
        "\n",
        "    loss = criterion(output, y_train_tensor)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    #if (epoch + 1) % 5 == 0:\n",
        "        #print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluar el modelo\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_tensor = model(X_test_tensor)\n",
        "    y_pred = y_pred_tensor.numpy()\n",
        "\n",
        "# Evaluación del modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Fine-tunning MSE: {mse}\")\n",
        "print(f\"Fine-tunning MAE: {mae}\")\n",
        "\n",
        "# Gráfico comparativo\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, color='green', alpha=0.6, label=\"Fine-Tuning BERT Predictions\")\n",
        "plt.plot(y_test, y_test, color='black', linestyle='--', label=\"Perfect Prediction (y=x)\")\n",
        "plt.xlabel(\"True Ratings\")\n",
        "plt.ylabel(\"Predicted Ratings\")\n",
        "plt.title(\"Fine-Tuning BERT Predictions\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "y_test_numpy = y_test_tensor.numpy()\n",
        "y_pred_knn_numpy = y_pred_knn\n",
        "y_pred_nn_numpy = y_pred_nn.numpy()\n",
        "y_pred_finetune_numpy = y_pred\n",
        "\n",
        "\n",
        "# 5. Comparación KNN, NN y Fine-Tuning BERT\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test_numpy, y_pred_knn_numpy, color='blue', alpha=0.6, label=\"KNN Predictions\")\n",
        "plt.scatter(y_test_numpy, y_pred_nn_numpy, color='red', alpha=0.6, label=\"NN Predictions\")\n",
        "plt.scatter(y_test_numpy, y_pred_finetune_numpy, color='green', alpha=0.6, label=\"Fine-Tuning BERT Predictions\")\n",
        "plt.plot([y_test_numpy.min(), y_test_numpy.max()], [y_test_numpy.min(), y_test_numpy.max()], color='black', linestyle='--', label=\"Perfect Prediction (y=x)\")\n",
        "plt.xlabel(\"True Ratings\")\n",
        "plt.ylabel(\"Predicted Ratings\")\n",
        "plt.title(\"Comparison: KNN vs NN vs Fine-Tuning BERT\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "############################################################################\n",
        "####################### EXTENSION ###########################################\n",
        "######################################################################\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Cargar los pipelines para resumen con tres modelos diferentes\n",
        "summarizer_bart = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "summarizer_t5 = pipeline(\"summarization\", model=\"t5-small\")\n",
        "summarizer_pegasus = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
        "\n",
        "# Funciones para resumir las instrucciones\n",
        "def summarize_instructions_bart(text):\n",
        "    input_length = len(text.split())\n",
        "    max_length = min(150, input_length)  # Ajustar max_length según el tamaño del texto\n",
        "    summary = summarizer_bart(text, max_length=max_length, min_length=50, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "def summarize_instructions_t5(text):\n",
        "    input_length = len(text.split())\n",
        "    max_length = min(150, input_length)  # Ajustar max_length según el tamaño del texto\n",
        "    summary = summarizer_t5(text, max_length=max_length, min_length=50, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "def summarize_instructions_pegasus(text):\n",
        "    input_length = len(text.split())\n",
        "    max_length = min(150, input_length)  # Ajustar max_length según el tamaño del texto\n",
        "    summary = summarizer_pegasus(text, max_length=max_length, min_length=50, do_sample=False)\n",
        "    return summary[0]['summary_text']\n",
        "\n",
        "recipes_df = recipes_df.head(3)\n",
        "\n",
        "# Resumir las instrucciones para todas las filas del DataFrame\n",
        "results = []\n",
        "for index, row in recipes_df.iterrows():\n",
        "    recipe_directions = row[\"directions\"]\n",
        "\n",
        "    # Generar resúmenes con cada modelo\n",
        "    summary_bart = summarize_instructions_bart(' '.join(recipe_directions))\n",
        "    summary_t5 = summarize_instructions_t5(' '.join(recipe_directions))\n",
        "    summary_pegasus = summarize_instructions_pegasus(' '.join(recipe_directions))\n",
        "\n",
        "    results.append({\n",
        "        'index': index,\n",
        "        'original': ' '.join(recipe_directions),\n",
        "        'summary_bart': summary_bart,\n",
        "        'summary_t5': summary_t5,\n",
        "        'summary_pegasus': summary_pegasus\n",
        "    })\n",
        "\n",
        "\n",
        "# Convertir los resultados a un DataFrame\n",
        "summary_df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(summary_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}