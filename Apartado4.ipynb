{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ9k_DuqolgG",
        "outputId": "3745d3d4-51ce-470d-eeeb-9466abde8239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos cargados exitosamente.\n",
            "Vectores promedio de Word2Vec:\n",
            "[[ 0.00017117 -0.00023709  0.00101066 ...  0.00053264 -0.00170498\n",
            "  -0.00023669]\n",
            " [-0.00340243  0.00443886 -0.00316347 ... -0.00335237  0.00099146\n",
            "   0.00302364]\n",
            " [-0.0067749   0.0021796  -0.00262231 ... -0.00672233 -0.00997708\n",
            "   0.00724622]\n",
            " ...\n",
            " [-0.00203047  0.00084168  0.00073573 ... -0.00206656 -0.00529526\n",
            "  -0.00232061]\n",
            " [ 0.00070065 -0.00098148  0.00109488 ... -0.00133838  0.00067744\n",
            "  -0.00143426]\n",
            " [-0.00234498 -0.00299736 -0.00030339 ... -0.00126365  0.00147225\n",
            "   0.00182848]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Inicializar spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "# Cargar el modelo BERT y el tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Asegurarse de que el modelo esté en modo de evaluación\n",
        "model.eval()\n",
        "\n",
        "# Ruta al archivo JSON (ajusta el path según donde esté ubicado el archivo)\n",
        "path_file = \"full_format_recipes.json\"\n",
        "\n",
        "# Cargar el archivo JSON como un DataFrame de Pandas\n",
        "def load_recipes(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\") as file:\n",
        "            recipes_df = pd.read_json(file)\n",
        "            recipes_df.dropna(inplace=True)\n",
        "            print(\"Datos cargados exitosamente.\")\n",
        "            return recipes_df\n",
        "    except ValueError as e:\n",
        "        print(f\"Error al leer el archivo JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text_spacy(text):\n",
        "    \"\"\"\n",
        "    Preprocesa el texto usando SpaCy:\n",
        "    - Elimina caracteres no alfabéticos\n",
        "    - Realiza tokenización y lematización\n",
        "    \"\"\"\n",
        "    # Eliminar caracteres no alfabéticos\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Procesar el texto con SpaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Tokenización y Lematización con SpaCy\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    text_lemma = ' '.join(tokens)\n",
        "\n",
        "    # Convertimos el texto en una lista de tokens\n",
        "    words = re.findall(r'\\S+', text_lemma)\n",
        "\n",
        "    # Devolver el texto procesado como lista de palabras\n",
        "    return words\n",
        "\n",
        "def compute_tfidf(docs):\n",
        "    \"\"\"\n",
        "    Calcula la representación TF-IDF de los documentos.\n",
        "    :param docs: Lista de documentos como texto\n",
        "    :return: Matriz TF-IDF y el vectorizador\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(docs)\n",
        "    return tfidf_matrix, vectorizer\n",
        "\n",
        "def compute_word2vec(docs):\n",
        "    \"\"\"\n",
        "    Calcula la representación vectorial de documentos usando Word2Vec.\n",
        "    :param docs: Lista de documentos como texto\n",
        "    :return: Lista de vectores promediados para cada documento\n",
        "    \"\"\"\n",
        "    # Tokenización de documentos\n",
        "    tokenized_docs = [doc.split() for doc in docs]\n",
        "\n",
        "    # Entrenamiento de Word2Vec\n",
        "    model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Calcular promedio de embeddings para cada documento\n",
        "    doc_vectors = []\n",
        "    for tokens in tokenized_docs:\n",
        "        word_vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "        if word_vectors:\n",
        "            doc_vectors.append(np.mean(word_vectors, axis=0))\n",
        "        else:\n",
        "            doc_vectors.append(np.zeros(model.vector_size))\n",
        "\n",
        "    return np.array(doc_vectors), model\n",
        "def compute_bert_embeddings(docs):\n",
        "    \"\"\"\n",
        "    Calcula los embeddings de BERT para cada documento.\n",
        "    :param docs: Lista de documentos como texto\n",
        "    :return: Lista de embeddings de documentos\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "\n",
        "    for doc in docs:\n",
        "        # Tokenizar el documento\n",
        "        inputs = tokenizer(doc, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "\n",
        "        # Pasar los tokens por BERT\n",
        "        with torch.no_grad():  # No necesitamos gradientes para la inferencia\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # Extraer los embeddings de la ´ultima capa oculta\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        # Tomamos la representación del token [CLS] (primer token)\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "\n",
        "        embeddings.append(cls_embedding)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Cargar recetas\n",
        "recipes_df = load_recipes(path_file)\n",
        "\n",
        "# Tomar una muestra de las primeras 20 filas para prueba\n",
        "recipes_df = recipes_df.iloc[0:100, :]\n",
        "\n",
        "# Obtener la columna 'desc' (descripción de las recetas)\n",
        "text_to_process = recipes_df.loc[:, \"desc\"]\n",
        "ratings = recipes_df.loc[:, \"rating\"]\n",
        "\n",
        "# Aplicar el preprocesamiento de texto a cada descripción\n",
        "processed_words = text_to_process.apply(preprocess_text_spacy)\n",
        "\n",
        "# Convertir la lista de palabras procesadas a un formato adecuado para TF-IDF y Word2Vec (como lista de strings)\n",
        "processed_docs = [' '.join(words) for words in processed_words]\n",
        "\n",
        "# 1. Aplicar TF-IDF\n",
        "tfidf_matrix, vectorizer = compute_tfidf(processed_docs)\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "matrix=tfidf_matrix.toarray()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print(\"Terminos\")\n",
        "print(terms)\n",
        "print(\"Matriz TF-IDF:\")\n",
        "for i, doc in enumerate(matrix):\n",
        "    print(i)\n",
        "    print(f\"recipe {i+1}: {doc}\")\n",
        "\"\"\"\n",
        "\n",
        "# 2. Aplicar Word2Vec\n",
        "word2vec_vectors, word2vec_model = compute_word2vec(processed_docs)\n",
        "print(\"Vectores promedio de Word2Vec:\")\n",
        "print(word2vec_vectors)\n",
        "\n",
        "plt.figure(2)\n",
        "# Plotear los vectores\n",
        "plt.scatter(word2vec_vectors[:, 0], word2vec_vectors[:, 1])\n",
        "plt.title(\"Representación de recetas con Word2Vec (PCA)\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Aplicar BERT\n",
        "# Convertir las descripciones a una lista de documentos (sin preprocesar)\n",
        "processed_docs = text_to_process.tolist()\n",
        "\n",
        "# Obtener los embeddings de BERT para las descripciones\n",
        "bert_embeddings = compute_bert_embeddings(processed_docs)\n",
        "\n",
        "\"\"\"\n",
        "# Mostrar los embeddings de BERT para los primeros 3 documentos\n",
        "print(\"Embeddings de BERT:\")\n",
        "print(bert_embeddings[:3])  # Muestra los primeros 3 embeddings\n",
        "\"\"\"\n",
        "\n",
        "#######################################\n",
        "#Redes Neuronales utilizando PyTorch\n",
        "#######################################\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 4. Entrenamiento y evaluaci´on de modelos de regresión\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(bert_embeddings, ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# Estandarizar los datos (si es necesario para KNN, ya que KNN depende de las distancias entre puntos)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#####\n",
        "###  Regresor K-NN  ###\n",
        "#####\n",
        "\n",
        "def KNN_Regresor(X_train_scaled,X_test_scaled,y_train,n_neighbors=5):\n",
        "\n",
        "    # Entrenamiento del modelo KNN Regressor\n",
        "    knn_model = KNeighborsRegressor(n_neighbors)  # Puedes ajustar 'n_neighbors' según sea necesario\n",
        "    knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predicción y evaluación del modelo\n",
        "    y_pred_knn = knn_model.predict(X_test_scaled)\n",
        "\n",
        "    # Calcular el error cuadrático medio (MSE) para KNN\n",
        "    mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "    print(f\"KNN Regressor MSE: {mse_knn}\")\n",
        "    return y_pred_knn\n",
        "\n",
        "y_pred_knn=KNN_Regresor(X_train_scaled,X_test_scaled,y_train,n_neighbors=5)\n",
        "\n",
        "######\n",
        "###  Regresión_NN con pytorch ###\n",
        "######\n",
        "# Definir el modelo de red neuronal\n",
        "class RegressionNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)  # Capa de entrada\n",
        "        self.fc2 = nn.Linear(128, 64)  # Capa oculta\n",
        "        self.fc3 = nn.Linear(64, 1)  # Capa de salida (valor de rating)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Convertir los datos a tensores de PyTorch\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Inicializar el modelo\n",
        "model_nn = RegressionNN(input_size=X_train_scaled.shape[1])\n",
        "\n",
        "# Definir la función de pérdida y el optimizador\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_nn.parameters(), lr=0.001)\n",
        "\n",
        "# Entrenamiento de la red neuronal\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model_nn(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    # Backward pass y optimización\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
        "\n",
        "# Predicción y evaluación del modelo\n",
        "model_nn.eval()  # Poner el modelo en modo evaluación\n",
        "with torch.no_grad():\n",
        "    y_pred_nn = model_nn(X_test_tensor)\n",
        "\n",
        "# Calcular el error cuadrático medio (MSE) para la red neuronal\n",
        "mse_nn = mean_squared_error(y_test_tensor.numpy(), y_pred_nn.numpy())\n",
        "print(f\"Neural Network MSE: {mse_nn}\")\n",
        "\n",
        "\n",
        "plt.figure(3)\n",
        "# Visualización de los resultados de la predicción vs los valores reales\n",
        "plt.scatter(y_test, y_pred_knn, color='blue', label=\"KNN Predictions\")\n",
        "plt.scatter(y_test, y_pred_nn, color='red', label=\"NN Predictions\")\n",
        "plt.xlabel(\"True Ratings\")\n",
        "plt.ylabel(\"Predicted Ratings\")\n",
        "plt.title(\"KNN vs Neural Network Predictions\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}